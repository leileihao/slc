{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leileihao/slc/blob/main/SleepClassTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Functions"
      ],
      "metadata": {
        "id": "8dfYVcqP_vNe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3x1w2eGq3Adu",
        "outputId": "9280621b-fbb0-48fb-c23b-5cb62975e059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pickle\n",
        "import scipy.io as so\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.transforms import Normalize\n",
        "from torch.autograd import Variable\n",
        "from pandas import Series\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/WeberLab/')\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwyFMDaJ8O7Y",
        "outputId": "df916e3e-e79d-4d00-b4f3-8e4f9f90991f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0+cu126\n",
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(46) # For consistency\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(torch.__version__)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcIa_rlU6D1N"
      },
      "outputs": [],
      "source": [
        "# Modified specifically to look at the 3_remix sleep state file\n",
        "def load_stateidx(ppath, name, ann_name=''):\n",
        "    \"\"\" load the sleep state file of recording (folder) $ppath/$name\n",
        "    @Return:\n",
        "        M,K         sequence of sleep states, sequence of\n",
        "                    0'1 and 1's indicating non- and annotated states\n",
        "    \"\"\"\n",
        "    ddir = os.path.join(ppath, name)\n",
        "    ppath, name = os.path.split(ddir)\n",
        "\n",
        "    if ann_name == '':\n",
        "        ann_name = name\n",
        "\n",
        "    remidxfile3 = os.path.join(ppath, name, '3_remidx_' + ann_name + '.txt')\n",
        "    remidxfile_regular = os.path.join(ppath, name, 'remidx_' + ann_name + '.txt')\n",
        "\n",
        "    # Check if '3_remidx_' file exists, if not use 'remidx_' file\n",
        "    if os.path.exists(remidxfile3):\n",
        "      remidxfile = remidxfile3\n",
        "      print('3_remdix')\n",
        "\n",
        "    else:\n",
        "      remidxfile = remidxfile_regular\n",
        "      print('remdix')\n",
        "\n",
        "\n",
        "    f = open(remidxfile, 'r')\n",
        "    lines = f.readlines()\n",
        "    f.close()\n",
        "\n",
        "    n = 0\n",
        "    for l in lines:\n",
        "        if re.match('\\d', l):\n",
        "            n += 1\n",
        "\n",
        "    M = np.zeros(n, dtype='int')\n",
        "    K = np.zeros(n, dtype='int')\n",
        "\n",
        "    i = 0\n",
        "    for l in lines :\n",
        "\n",
        "        if re.search('^\\s+$', l) :\n",
        "            continue\n",
        "        if re.search('\\s*#', l) :\n",
        "            continue\n",
        "\n",
        "        if re.match('\\d+\\s+-?\\d+', l) :\n",
        "            a = re.split('\\s+', l)\n",
        "            M[i] = int(a[0])\n",
        "            K[i] = int(a[1])\n",
        "            i += 1\n",
        "\n",
        "    return M,K\n",
        "\n",
        "\n",
        "# Data loader for stage 1\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, data, labels, window_size):\n",
        "    self.data = torch.tensor(data, dtype=torch.float32)\n",
        "    self.labels = torch.tensor(labels, dtype=torch.uint8)\n",
        "    self.window_size = window_size\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data) // self.window_size\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    x = self.data[index*window_size:index*window_size+window_size][:]\n",
        "    label_index = min(index * self.window_size, len(self.labels) - 1)\n",
        "    y = self.labels[label_index]\n",
        "    y = y.long()\n",
        "\n",
        "    # Perform one-hot encoding on y\n",
        "    y = y - 1 # shift the labels to start at 0\n",
        "    y_encoded = F.one_hot(y, num_classes = 3)\n",
        "\n",
        "    x = x.view(-1, 2)\n",
        "    return x, y_encoded\n",
        "\n",
        "\n",
        "# Data loader for stage 2\n",
        "class CustomDatasetFC(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = torch.tensor(data, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.labels[index]\n",
        "\n",
        "        return x, y\n",
        "\n",
        "# Create segments\n",
        "def create_segments(array, segment_length, overlap):\n",
        "    segments = []\n",
        "    stride = segment_length - overlap\n",
        "    for i in range(0, len(array) - segment_length + 1, stride):\n",
        "        segments.append(array[i:i+segment_length])\n",
        "    return segments\n",
        "\n",
        "segment_length = 51\n",
        "overlap = 50\n",
        "\n",
        "\n",
        "def load_features(name, file_type, custom_length=False):\n",
        "  output = so.loadmat(os.path.join(ppath, name, '{}.mat'.format(file_type)), squeeze_me=True)[file_type]\n",
        "  output = output[:(len(output) // factor) * factor]\n",
        "\n",
        "  if custom_length:\n",
        "    output = output[start_index*ndown: start_index*ndown + sample_len]\n",
        "    output = output[: sample_len]\n",
        "\n",
        "  output = sleepy.downsample_vec(output, ndown)\n",
        "  output = output.astype('float16')\n",
        "  return output\n",
        "\n",
        "def load_labels(name, custom_length=False):\n",
        "  m = load_stateidx(ppath, name)[0]\n",
        "  m = m.astype('i')\n",
        "  m = m[:-1]\n",
        "  if custom_length:\n",
        "    m = m[:int(sample_len / factor)]\n",
        "\n",
        "  for i in range(len(m)):\n",
        "    if m[i] > 3:\n",
        "      m[i] = 3\n",
        "  return m\n",
        "\n",
        "def train_test_split(data):\n",
        "  x_indices = list(range(len(data)))\n",
        "  x_split = int(np.floor(validation_split * len(data)))\n",
        "\n",
        "  x_train_indices, x_val_indices = x_indices[x_split:], x_indices[:x_split]\n",
        "\n",
        "  x_train = data[x_train_indices]\n",
        "  x_test = data[x_val_indices]\n",
        "\n",
        "  return x_train, x_test\n",
        "\n",
        "def my_bpfilter(x, w0, w1, N=4,bf=True):\n",
        "    \"\"\"\n",
        "    create N-th order bandpass Butterworth filter with corner frequencies\n",
        "    w0*sampling_rate/2 and w1*sampling_rate/2\n",
        "    \"\"\"\n",
        "    #from scipy import signal\n",
        "    #taps = signal.firwin(numtaps, w0, pass_zero=False)\n",
        "    #y = signal.lfilter(taps, 1.0, x)\n",
        "    #return y\n",
        "    from scipy import signal\n",
        "    b,a = signal.butter(N, [w0, w1], 'bandpass')\n",
        "    if bf:\n",
        "        y = signal.filtfilt(b,a, x)\n",
        "    else:\n",
        "        y = signal.lfilter(b,a, x)\n",
        "\n",
        "    return y\n",
        "\n",
        "def get_sr(ppath, name):\n",
        "    \"\"\"\n",
        "    read and return sampling rate (SR) from the info.txt file $ppath/$name/info.txt\n",
        "    \"\"\"\n",
        "    fid = open(os.path.join(ppath, name, 'info.txt'), newline=None)\n",
        "    lines = fid.readlines()\n",
        "    fid.close()\n",
        "    values = []\n",
        "    for l in lines :\n",
        "        a = re.search(\"^\" + 'SR' + \":\" + \"\\s+(.*)\", l)\n",
        "        if a :\n",
        "            values.append(a.group(1))\n",
        "    return float(values[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4ZBZy7SFMuR"
      },
      "outputs": [],
      "source": [
        "from torch import sparse\n",
        "# Bidirectional LSTM\n",
        "class biLSTM(nn.Module):\n",
        "  def __init__(self, input_size, window_len, hidden_size=128, num_layers=2, dropout=0.2):\n",
        "    super(biLSTM, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
        "                            dropout=dropout, batch_first=True, bidirectional=True)\n",
        "    self.fc = nn.Sequential(\n",
        "          nn.BatchNorm1d(hidden_size*2),\n",
        "          nn.Linear(hidden_size*2, hidden_size),\n",
        "          nn.ReLU(),\n",
        "          nn.BatchNorm1d(hidden_size),\n",
        "          nn.Linear(hidden_size, 64),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(64, 3), # three brain states: 1, 2, 3 (--> now 0, 1, 2 due to shift)\n",
        "    )\n",
        "\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, seq_len, num_channels = x.size()\n",
        "\n",
        "    h0 = torch.randn(2 * self.num_layers, batch_size, self.hidden_size).double().to(device)\n",
        "    c0 = torch.randn(2 * self.num_layers, batch_size, self.hidden_size).double().to(device)\n",
        "\n",
        "    x = x.permute(0, 1, 2)  # Transpose the tensor to match the shape (batch, seq_len, num_channels)\n",
        "\n",
        "    outl, _ = self.lstm(x, (h0, c0))\n",
        "    out = outl[:, -1, :]\n",
        "\n",
        "    out = self.fc(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "class FCNet(nn.Module):\n",
        "    def __init__(self, dropout=0.2):\n",
        "        super(FCNet, self).__init__()\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(segment_length * 3, 64), # ADD TO SEG LEGTH IF WE ARE LOOKING AT SPEC\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.Linear(32, 3),\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        # out = self.fc(x)\n",
        "        out = self.fc(x) + x[:,int((segment_length-1)/2+0.1)]   ## (segment_length-1)/2\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def time_morph(X, nstates):\n",
        "    \"\"\"\n",
        "    upsample vector or matrix X to nstates states\n",
        "    :param X, vector or matrix; if matrix, the rows are upsampled.\n",
        "    :param nstates, number of elements or rows of returned vector or matrix\n",
        "    I want to upsample m by a factor of x such that\n",
        "    x*m % nstates == 0,\n",
        "    a simple soluation is to set x = nstates\n",
        "    then nstates * m / nstates = m.\n",
        "    so upsampling X by a factor of nstates and then downsampling by a factor\n",
        "    of m is a simple solution...\n",
        "    \"\"\"\n",
        "    m = X.shape[0]\n",
        "    A = upsample_mx(X, nstates)\n",
        "    # now we have m * nstates rows\n",
        "    if X.ndim == 1:\n",
        "        Y = downsample_vec(A, int((m * nstates) / nstates))\n",
        "    else:\n",
        "        Y = downsample_mx(A, int((m * nstates) / nstates))\n",
        "    # now we have m rows as requested\n",
        "    return Y\n",
        "\n",
        "def upsample_mx(x, nbin):\n",
        "    \"\"\"\n",
        "    if x is a vector:\n",
        "        upsample the given vector $x by duplicating each element $nbin times\n",
        "    if x is a 2d array:\n",
        "        upsample each matrix by duplication each row $nbin times\n",
        "    \"\"\"\n",
        "    if nbin == 1:\n",
        "        return x\n",
        "\n",
        "    nelem = x.shape[0]\n",
        "    if x.ndim == 1:\n",
        "        y = np.zeros((nelem * nbin,))\n",
        "        for k in range(nbin):\n",
        "            y[k::nbin] = x\n",
        "    else:\n",
        "        y = np.zeros((nelem * nbin, x.shape[1]))\n",
        "        for k in range(nbin):\n",
        "            y[k::nbin, :] = x\n",
        "\n",
        "    return y\n",
        "\n",
        "def downsample_mx(X, nbin):\n",
        "    \"\"\"\n",
        "    y = downsample_vec(x, nbin)\n",
        "    downsample the vector x by replacing nbin consecutive\n",
        "    bin by their mean\n",
        "    @RETURN: the downsampled vector\n",
        "    \"\"\"\n",
        "    n_down = int(np.floor(X.shape[0] / nbin))\n",
        "    X = X[0:n_down * nbin, :]\n",
        "    X_down = np.zeros((n_down, X.shape[1]))\n",
        "\n",
        "    # 0 1 2 | 3 4 5 | 6 7 8\n",
        "    for i in range(nbin):\n",
        "        idx = list(range(i, int(n_down * nbin), int(nbin)))\n",
        "        X_down += X[idx, :]\n",
        "\n",
        "    return X_down / nbin\n",
        "\n",
        "def downsample_vec(x, nbin):\n",
        "    \"\"\"\n",
        "    y = downsample_vec(x, nbin)\n",
        "    downsample the vector x by replacing nbin consecutive \\\n",
        "    bin by their mean \\\n",
        "    @RETURN: the downsampled vector\n",
        "    \"\"\"\n",
        "    n_down = int(np.floor(len(x) / nbin))\n",
        "    x = x[0:n_down*nbin]\n",
        "    x_down = np.zeros((n_down,))\n",
        "\n",
        "    # 0 1 2 | 3 4 5 | 6 7 8\n",
        "    for i in range(nbin) :\n",
        "        idx = list(range(i, int(n_down*nbin), int(nbin)))\n",
        "        x_down += x[idx]\n",
        "\n",
        "    return x_down / nbin"
      ],
      "metadata": {
        "id": "hg2hJu3dXVCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and Evaluate"
      ],
      "metadata": {
        "id": "u75sd3VVAP62"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9s3mhWh9G27o"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "class CPU_Unpickler(pickle.Unpickler):\n",
        "    def find_class(self, module, name):\n",
        "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
        "            return lambda b: torch.load(io.BytesIO(b), map_location=device)\n",
        "        else: return super().find_class(module, name)\n",
        "\n",
        "biLSTM_model = CPU_Unpickler(open('/content/drive/MyDrive/MultiVanillaModelPart1N.pkl',\"rb\")).load()\n",
        "fc_model = CPU_Unpickler(open('/content/drive/MyDrive/MultiVanillaModelPart2Ov2.pkl',\"rb\")).load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2rI4wIjHs7O",
        "outputId": "ab2cbdc7-d6ec-45a1-c04e-283252be82b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FCNet(\n",
              "  (fc): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=153, out_features=64, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.2, inplace=False)\n",
              "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Dropout(p=0.2, inplace=False)\n",
              "    (7): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (8): Linear(in_features=32, out_features=32, bias=True)\n",
              "    (9): ReLU()\n",
              "    (10): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (11): Linear(in_features=32, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "fc_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TK5LSbbHHDI",
        "outputId": "9c539033-a09f-4cfb-b9a1-fd6e72440c1a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "biLSTM(\n",
              "  (lstm): LSTM(2, 128, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
              "  (fc): Sequential(\n",
              "    (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (1): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=64, out_features=3, bias=True)\n",
              "  )\n",
              "  (softmax): Softmax(dim=1)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "biLSTM_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRmoinqK2XEN"
      },
      "outputs": [],
      "source": [
        "# Concatenate the EEG, EMG, and sleep state (M) of the list of recordings\n",
        "ppath = '/content/drive/MyDrive/SleepClass/heart_rate_new'\n",
        "recordings = os.listdir(ppath)\n",
        "\n",
        "recordings = ['JW34_072925n2', 'JW32_080225n2', 'JW34_080225n2', 'JW31_080225n2',\n",
        "              'JW32_072925n2', 'JW16_050525n2', 'JW31_072925n2']\n",
        "ndown = 4\n",
        "factor = 2500\n",
        "for name in recordings:\n",
        "  m = load_stateidx(ppath, name)[0]\n",
        "  m = m.astype('i')\n",
        "  m = m[:-1] # cut off the end that doesn't match\n",
        "  len_m = len(m)\n",
        "\n",
        "  eeg = so.loadmat(os.path.join(ppath, name, 'EEG.mat'), squeeze_me=True)['EEG']\n",
        "  eeg = eeg[:(len_m) * factor]    # truncate eeg length to the number of windows\n",
        "  eeg = downsample_vec(eeg, ndown)\n",
        "  eeg = eeg.astype('float16')\n",
        "\n",
        "  emg = so.loadmat(os.path.join(ppath, name, 'EMG.mat'), squeeze_me=True)['EMG']\n",
        "  emg = emg[:(len_m) * factor]\n",
        "  emg = downsample_vec(emg, ndown)\n",
        "  emg = emg.astype('float16')\n",
        "\n",
        "  # Change all the sleep states above 3 to NREM -- can change to separate out microarousals\n",
        "  for i in range(len(m)):\n",
        "    if m[i] > 3:\n",
        "      m[i] = 3\n",
        "    if m[i] == 0: # changed the unannotated segments to NREM\n",
        "      m[i] = 3\n",
        "      print(\"There are weird annotations in here.\")\n",
        "\n",
        "  # Prepare the data for testing\n",
        "  m_upsampled = np.repeat(m, int(factor/ndown))\n",
        "  x_input = np.vstack((eeg, emg)).T\n",
        "  input = StandardScaler().fit_transform(x_input)\n",
        "\n",
        "  ### STAGE 1\n",
        "  window_size = 625\n",
        "  input_size = 2\n",
        "  train_dataset = CustomDataset(input, m_upsampled, window_size=window_size)\n",
        "  batch_size = 128\n",
        "  loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  biLSTM_model.eval()\n",
        "  test_predicted_labels = []\n",
        "  test_real_labels = []\n",
        "  val_correct = 0\n",
        "  val_total = 0\n",
        "  with torch.no_grad():\n",
        "    for iter, (input, target) in enumerate(loader):\n",
        "          input = input.to(device).double()\n",
        "          target = target.to(device).double()\n",
        "\n",
        "          output = biLSTM_model(input)\n",
        "\n",
        "          test_predicted_labels.extend(output.tolist())\n",
        "          test_real_labels.extend(target.tolist())\n",
        "\n",
        "  test_predicted_labels = torch.tensor(test_predicted_labels)\n",
        "  test_real_labels = torch.tensor(test_real_labels)\n",
        "\n",
        "  val_output = F.one_hot(torch.max(test_predicted_labels, dim=1, keepdim=True)[1], num_classes = 3).squeeze()\n",
        "  if val_output.dim() == 1:\n",
        "    predicted_classes = torch.argmax(val_output, dim=0)\n",
        "  else:\n",
        "    predicted_classes = torch.argmax(val_output, dim=1)\n",
        "\n",
        "  predicted_classes = torch.tensor(predicted_classes)\n",
        "  val_correct = (predicted_classes == torch.argmax(test_real_labels, dim=1)).sum().item()\n",
        "  val_total = test_real_labels.size(0)\n",
        "\n",
        "  val_accuracy = 100 * val_correct / val_total\n",
        "\n",
        "  # Stage 2\n",
        "  idx = int(overlap/2)\n",
        "  final_segments = np.array(create_segments(test_predicted_labels,\n",
        "                           segment_length, overlap))\n",
        "  final_labels = test_real_labels[idx:len(test_real_labels) - idx]\n",
        "\n",
        "  # final_segments = StandardScaler().fit_transform(final_segments.reshape(len(final_segments),-1)).reshape(-1, segment_length, 3)\n",
        "\n",
        "  # def minmax_scale(x, axis=None):\n",
        "  #   min = np.min(x, axis=axis, keepdims=True)\n",
        "  #   max = np.max(x, axis=axis, keepdims=True)\n",
        "  #   result = (x-min)/(max-min)\n",
        "  #   return result\n",
        "\n",
        "  # def zscore_scale(x, axis=None):\n",
        "  #   mean = np.mean(x, axis=axis, keepdims=True)\n",
        "  #   std = np.std(x, axis=axis, keepdims=True)\n",
        "  #   std = np.where(std == 0, 1, std)\n",
        "  #   result = (x-mean)/std\n",
        "  #   return result\n",
        "\n",
        "  # final_segments = zscore_scale(final_segments, axis=2)\n",
        "  axis = (0, 2)   ## this is to ensure the same timepoint is divided by the same mean, while this mean is representing the whole trainig set\n",
        "  mean_x = np.nanmean(final_segments, axis=axis, keepdims=True)\n",
        "  std_x = np.nanstd(final_segments, axis=axis, keepdims=True)\n",
        "  final_segments = ((final_segments - mean_x) / std_x).astype('<f8')\n",
        "\n",
        "  # Fully connected layer data loader\n",
        "  test_dataset = CustomDatasetFC(final_segments, final_labels)\n",
        "\n",
        "  # Create data loaders\n",
        "  batch_size = 128\n",
        "  test_loader_fc = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  fc_model.eval()\n",
        "  final_test_predicted_labels = []\n",
        "  final_test_real_labels = []\n",
        "  val_correct = 0\n",
        "  val_total = 0\n",
        "  with torch.no_grad():\n",
        "      for iter, (input, target) in enumerate(test_loader_fc):\n",
        "          input = input.to(device).double()\n",
        "          target = target.to(device).double()\n",
        "\n",
        "          val_output = fc_model(input)\n",
        "\n",
        "          val_output_classes = torch.argmax(val_output, dim=1) + 1\n",
        "          final_test_predicted_labels.extend(val_output_classes.tolist())\n",
        "          target_classes = torch.argmax(target, dim=1) + 1\n",
        "          final_test_real_labels.extend(target_classes.tolist())\n",
        "\n",
        "\n",
        "          val_output = F.one_hot(torch.max(val_output, dim=1, keepdim=True)[1], num_classes = 3).squeeze()\n",
        "          val_correct += (torch.argmax(val_output, dim=1) == torch.argmax(target, dim=1)).sum().item()\n",
        "  final_test_real_labels = torch.tensor(final_test_real_labels)\n",
        "  val_total = final_test_real_labels.size(0)\n",
        "\n",
        "  fc_val_accuracy = 100 * val_correct / val_total\n",
        "\n",
        "\n",
        "  print(\"Test Subject: {} | biLSTM Accuracy: {} | FC Accuracy: {}\"\n",
        "          .format(name, val_accuracy, fc_val_accuracy))\n",
        "\n",
        "  y_true = final_test_real_labels\n",
        "  y_pred = final_test_predicted_labels\n",
        "\n",
        "  cf_matrix = confusion_matrix(y_true, y_pred, labels=[1, 2, 3])\n",
        "  xticks = ['REM', 'Wake', 'NREM']\n",
        "  yticks = ['REM', 'Wake', 'NREM']\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(5, 5))\n",
        "  fig = sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True,\n",
        "            fmt='.2%', cmap='Blues', cbar=False, xticklabels=xticks, yticklabels=yticks)\n",
        "\n",
        "  ax.set_xlabel('Predicted', fontsize=18)\n",
        "  ax.set_ylabel('Actual', fontsize=18)\n",
        "  plt.title('Confusion Matrix for ' + name, fontsize=22)\n",
        "\n",
        "# the predicted results are in predicted_classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66In0myO0jr9"
      },
      "outputs": [],
      "source": [
        "y_true = final_test_real_labels\n",
        "y_pred = final_test_predicted_labels\n",
        "\n",
        "cf_matrix = confusion_matrix(y_true, y_pred, labels=[1, 2, 3])\n",
        "xticks = ['REM', 'Wake', 'NREM']\n",
        "yticks = ['REM', 'Wake', 'NREM']\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "# fig = sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True,\n",
        "#             fmt='.2%', cmap='Blues', cbar=False, xticklabels=xticks, yticklabels=yticks)\n",
        "fig = sns.heatmap(cf_matrix, annot=True,\n",
        "                  cmap='Reds', cbar=False, xticklabels=xticks, yticklabels=yticks)\n",
        "\n",
        "#fig.set(xlabel =\"Predicted\", ylabel = \"Actual\", title ='Confusion Matrix')\n",
        "\n",
        "ax.set_xlabel('Predicted', fontsize=18)\n",
        "ax.set_ylabel('Actual', fontsize=18)\n",
        "plt.title('Confusion Matrix', fontsize=22)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDriI90CgZ79"
      },
      "outputs": [],
      "source": [
        "# scaled_labels = [4] * idx + final_test_predicted_labels + [4] * (idx + 2) # Used for visualization on sleep annotation\n",
        "# K = [0] * len(scaled_labels)\n",
        "# s = [\"%d\\t%d\\n\" % (i,j) for (i,j) in zip(scaled_labels,K)]\n",
        "\n",
        "# direct = ppath + '/' + name + '/remidx_lstm_' + name + '.txt'\n",
        "# f = open(direct, 'w')\n",
        "# f.writelines(s)\n",
        "# f.close()\n",
        "\n",
        "# print(\"Annotations saved.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}